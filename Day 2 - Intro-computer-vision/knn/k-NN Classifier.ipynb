{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors Classifier (k-NN)\n",
    "\n",
    "O **k-NN** é o classificador mais simples na área de aprendizado de máquina. Diferentemente das redes neurais, não se realiza de fato um **aprendizado**; em vez disso, o algoritmo verifica a distância entre o objeto a ser classificado e os vetores de característica. Devido a sua simplicidade, é bastante utilizado em *benchmarks* de classificadores mais complexos como Artificial Neural Networks (**ANN**) e Suport Vector Machines (**SVM**).\n",
    "\n",
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"https://amueller.github.io/applied_ml_spring_2017/images/classifier_comparison.png\" alt=\"Classifier Comparison\">\n",
    "        <figcaption>Figura 1 - Comparação entre os classificadores (https://amueller.github.io/applied_ml_spring_2017/images/classifier_comparison.png).</figcaption>\n",
    "    </figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução\n",
    "\n",
    "O **k-NN** não passa por um processo de aprendizagem como o **ANN** e **SVM**, contudo existe um mecanismo de *mapeamento* que servirá para criar o modelo utilizado na classificação dos dados. Este modelo demanda a representação do objeto a ser classificado como uma lista de **descritores**, os quais são características que buscam representar um padrão para cada classe.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://ars.els-cdn.com/content/image/1-s2.0-S1568494617305859-gr2.jpg\" width=\"450\" height=\"450\" style=\"float:left\">\n",
    "    <img src=\"https://csdl-images.computer.org/trans/tp/1996/06/figures/i06483.gif\"  width=\"400\" height=\"400\" style=\"float:right\">\n",
    "    <figcaption style=\"clear:both\">Figura 2 - Exemplos de descritores (vetores de características) de imagens.</figcaption>\n",
    "</center>\n",
    "\n",
    "A Figura 2 mostra exemplos de vetores de características sobre imagens, onde cada pixel se torna um elemento do vetor. Portanto, cada imagem será representada por seu vetor característico correspondente, o qual fará parte de um conjunto de vetores que serão utilizados no processo de classificação. Além desse tipo de vetor de características, os elementos desses vetores podem ser nominais ou reais, como: **cor, preço, ano, altura, peso, estado civil, etc**. Outra informação relevante é que cada vetor possui uma **classe** associada, o qual servirá de referência para classificar os dados. A Figura 3 mostra a forma geral de um conjunto de descritores e suas repectivas classes. \n",
    "\n",
    "<center>\n",
    "    <img src=\"http://www.big-data.tips/wp-content/uploads/2016/08/textdata-features.jpg\" width=\"500\"/>\n",
    "    <figcaption>Figura 3 - Conjunto de descritores genérico.</figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Obtenção dos dados\n",
    "\n",
    "Neste problema será utilizado o *dataset* <a href=\"http://archive.ics.uci.edu/ml/datasets/Iris\">Iris</a>, que consiste em um conjunto de dados que visa classificar os tipos de flores Íris em **Setosa, Versicolour e Virginica**. Esse *dataset* é composto por 150 instâncias, sendo 50 para cada classe. Estes descritores são formados por 4 atributos: **tamanho da sépala, largura da sépala, tamanho da pétala** e **largura da pétala**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações das bibliotecas\n",
    "import pandas as pd\n",
    "from math import sqrt,floor\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Princípio de funcionamento\n",
    "\n",
    "A ideia básica do classificador **k-NN** está em medir a *distância* entre o indivíduo a ser classificado e os descritores, onde a classe atribuida a esse indivíduo será a mesma que a maioria dos **k** descritores mais próximos. Existem vários cálculos de distâncias que podem ser utilizados como métricas para encontrar os descritores mais próximos, sendo as mais conhecidas a *Distância Euclidiana* e a *Distância Manhattan*. \n",
    "\n",
    "<center>\n",
    "    <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/795b967db2917cdde7c2da2d1ee327eb673276c0\" width=\"450\">\n",
    "    <figcaption style=\"clear:both\">Equação 1 - Fórmula da Distância Euclidiana.</figcaption>\n",
    "    <img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/02436c34fc9562eb170e2e2cfddbb3303075b28e\"  width=\"400\">\n",
    "    <figcaption style=\"clear:both\">Equação 2 - Fórmula da Distância Manhattan.</figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Criando as funções para cálculos de distâncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(p, q):\n",
    "    if len(p) != len (q):\n",
    "        return -1\n",
    "    \n",
    "    local_sum = 0\n",
    "    for i in range(0, len(p)):\n",
    "        local_sum += pow(q[i] - p[i], 2)\n",
    "    \n",
    "    return sqrt (local_sum)\n",
    "\n",
    "def manhattan(p, q):\n",
    "    if len(p) != len (q):\n",
    "        return -1\n",
    "    \n",
    "    local_sum = 0\n",
    "    for i in range(0, len(p)):\n",
    "        local_sum += abs(p[i] - q[i])\n",
    "    \n",
    "    return local_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processamento\n",
    "\n",
    "O processo geral de implementação do **k-NN** segue as seguintes etapas:\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <strong>Etapa 1: </strong>\n",
    "        Obter os dados, assim como verificar a precisão deles, e realizar correções e remoção de dados desnecessários.\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>Etapa 2: </strong>\n",
    "        Separar o conjunto de dados em 2 conjuntos: <i>treino</i> e <i>testes</i>, sendo o primeiro composto por cerca de 60%-85% do total, e o segundo com o restante.\n",
    "    </li>\n",
    "    <li>\n",
    "        <strong>Etapa 3: </strong>\n",
    "        Realizar a classificação, seguindo o princípio de funcionamento descrito acima.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Obtenção dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando o dataset\n",
    "dataset = pd.read_csv(\"dataset/iris/dataset.csv\", header=None)\n",
    "\n",
    "# Índice das classes\n",
    "class_column = len (dataset.columns) - 1\n",
    "\n",
    "# Checando os dados\n",
    "print (dataset)\n",
    "\n",
    "# Lista com os nomes das classes\n",
    "class_names = pd.unique(dataset[class_column])\n",
    "\n",
    "# Descobrindo o número de instâncias por classes; e\n",
    "# Definindo número mínimo de classes\n",
    "min_classes = len(dataset)\n",
    "for i in class_names:\n",
    "    print( str(i) + ': ' + str(len (dataset.loc[dataset[class_column] == i])) )\n",
    "    min_classes = min(min_classes, len (dataset.loc[dataset[class_column] == i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Separação dos conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.7\n",
    "\n",
    "# Obtendo os conjuntos de treino e de testes\n",
    "\n",
    "trainset = dataset.loc[dataset[class_column] == class_names[0]][0:floor(train_percentage * min_classes)]\n",
    "testset  = dataset.loc[dataset[class_column] == class_names[0]][floor(train_percentage * min_classes):]\n",
    "\n",
    "for i in range(1,len(class_names)):\n",
    "    trainset = pd.concat([trainset, dataset.loc[dataset[class_column] == class_names[i]][0:floor(train_percentage * min_classes)]])\n",
    "    testset  = pd.concat([testset,  dataset.loc[dataset[class_column] == class_names[i]][floor(train_percentage * min_classes):]])\n",
    "\n",
    "print(\"Tamanho trainset: \" + str(len(trainset)))\n",
    "print(\"Tamanho testset: \" + str(len(testset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função de classificação\n",
    "def knn(k, train, element,function):\n",
    "    distance = []\n",
    "    \n",
    "    local_class_column = len (train.columns) - 1\n",
    "    \n",
    "    for _, row in train.iterrows():\n",
    "        distance.append((function(row[0:local_class_column], element[0:local_class_column]), row[local_class_column]))\n",
    "    \n",
    "    distance = sorted(distance)\n",
    "    distance = [classes[1] for classes in distance[0:k]]\n",
    "    \n",
    "    most_common = Counter(distance)\n",
    "    return max(most_common, key=most_common.get)\n",
    "\n",
    "# Função de avaliação de acurácia\n",
    "def evaluate(k, train, test, function):\n",
    "    acc = 0\n",
    "    \n",
    "    local_class_column = len (train.columns) - 1\n",
    "    \n",
    "    for _, row in test.iterrows():\n",
    "        if( knn(k, train, row, function) == row[local_class_column] ):\n",
    "            acc += 1\n",
    "    \n",
    "    return acc / len(test)\n",
    "\n",
    "# Descobrindo a acurácia para todas as configurações possíveis\n",
    "def evaluate_by_config(train, test, function):\n",
    "    for k in range(1, min_classes + 1):\n",
    "        print(\"K = \" + str(k) + \", acc = \" + str(evaluate(k, train, test, function)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando a melhor configuração\n",
    "evaluate_by_config(trainset, testset,euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Reduzindo os dados e mantendo acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma cópia dos dados\n",
    "cp_dataset = dataset.copy()\n",
    "\n",
    "# Alterando a label da classe para um número\n",
    "index = 1\n",
    "for i in class_names:\n",
    "    cp_dataset.loc[cp_dataset[class_column] == i, 4] = index\n",
    "    index += 1\n",
    "    \n",
    "# Novo dataset\n",
    "print(cp_dataset)\n",
    "\n",
    "# Correlação entre os dados\n",
    "print(\"\\nCorrelação: \")\n",
    "print(cp_dataset.corr())\n",
    "\n",
    "# Covariância entre os dados\n",
    "print(\"\\nCovariância: \")\n",
    "print(cp_dataset.cov())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novos datasets de treino e teste com apenas os descritores 2 e 3, além da classe\n",
    "new_trainset = trainset.iloc[:,2:]\n",
    "new_testset  = testset.iloc[:,2:]\n",
    "\n",
    "# Ajustando os índices das colunas\n",
    "new_trainset.columns = range(new_trainset.shape[1])\n",
    "new_testset.columns = range(new_testset.shape[1])\n",
    "\n",
    "# Checando a acurácia\n",
    "evaluate_by_config(new_trainset, new_testset, euclidean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "[1] Kevin Zakka. A Complete Guide to K-Nearest-Neighbors with Applications in Python and R, Available at: https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/ (Accessed: 28th March 2018).\n",
    "\n",
    "[2] Maxwell. Aprendizado de máquina - conceitos básicos, Available at: https://www.maxwell.vrac.puc-rio.br/25796/25796_4.PDF (Accessed: 28th March 2018).\n",
    "\n",
    "[3] Wikipedia (24th February 2018) k-nearest neighbors algorithm, Available at: https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm (Accessed: 28th March 2018)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
